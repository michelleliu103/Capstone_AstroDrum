<!doctype html>
<html>
<head>
<meta charset="UTF-8">
<title>details</title>
	<!--
<style type="text/css">
@import url("style_details.css");
</style>  -->
	
<style type="text/css">

html,8
html * {
  box-sizing: border-box;
  margin: 0;
  padding: 0;
}

body {
	font-family: arial, sans-serif;
	font-size: 1vw;
	background: #2d2d2d;
}

header {
  padding: 20px;
  z-index: 1000;
  top: 0;
  left: 0;
  width: 95%;
  background:rgba(0,0,0,0.2);
  font-family: Impact, Charcoal, sans-serif;
  font-size:36px;
  background-image:-moz-linear-gradient( 135deg, rgb(160,55,220) 0%, rgb(83,200,234) 100%);
  background-image: -webkit-linear-gradient( 135deg, rgb(160,55,220) 0%, rgb(83,200,234) 100%);
  background-image: -ms-linear-gradient( 135deg, rgb(160,55,220) 0%, rgb(83,200,234) 100%);
  letter-spacing: 4px;
	position: fixed;
}
	
	nav {
  z-index: 400;
  top: 0;
  left: 0;
  opacity: .9;
  position: relative;
  display: block !important;
  float: right;
  width: 33%;
  padding: 0px;
  height: 40px;
  margin: 10px;
}
nav ul li {
	float: left;
	color: #FFFFFF;
	font-size: 20px;
	text-align: center;
	margin-right: 25px;
	letter-spacing: 2px;
	font-weight: bold;
	transition: all 0.3s linear;
}
ul li a {
	color: #FFFFFF;
	text-decoration: none;
}
ul li:hover a {
	color: #071A1F;
}

	
.a {
 ach position: relative;
  color: #343144;
  background: #fff;
  height: 150px;
  padding: 4em 4em 4em;
  background-color: #2d2d2d;
  padding-top: 50px;
  padding-bottom: 50px;
}
	
.approach_header {
	color: #FFFFFF;
	text-align: center;
	margin: 50px;
	letter-spacing: 2px;
	font-family: Cambria, "Hoefler Text", "Liberation Serif", Times, "Times New Roman", serif;

}
	
.LeftTextArea {
    font-family: Cambria, "Hoefler Text", "Liberation Serif", Times, "Times New Roman", serif;
    font-size: 18px;
	width: 90%;
	margin: 20px;
	color: #FFFFFF;
}
.rightTextArea {
    font-family: Cambria, "Hoefler Text", "Liberation Serif", Times, "Times New Roman", serif;
    font-size: 12px;
}
</style>
</head>

	<header> Background and Concepts
	
			<nav>       
			<ul>
        <li><a href="index.html">Back to Homepage</a></li>
      </ul>	</nav>
	
	</header>
	
	 <section class="approach" id="approach">
    <h2 class="approach_header">Approach </h2>
		 <div class="LeftTextArea" align="left">
		<p>
			In order to supply our application with an infinite amount of sounds, we wanted to implement a means to generate synthetic percussion sounds which sounded as natural and unique as possible.  To do so, we utilized and expanded upon an exisitng research project WaveGAN.
		</p>
	</div>
  </section>
	
		 <section class="approach" id="wavegan">
    <h2 class="approach_header">WaveGAN </h2>
		 <div class="LeftTextArea" align="left">
		<p>
			WaveGAN is a research project for using unlabeled data in machine learning with the applicaiton of GANs (Generative Adversarial Networks) to generate synthetic sounds.</p>
		<p>In our application we will be incorporating WaveGAN into our project for the training and generation of synthetic percussion instruments.
		</p>
	</div>
  </section>
	
<body>


<div class="rightTextArea" align="middle">
		<img src="images/gan_architecture.jpg" width="478" height="282" alt=""/>
	</div>

	<div class="LeftTextArea" align="left">
		<p>The WaveGAN model is composed of two major components the generator and the discriminator, which make up the adversarial component of the GAN.  Within the discriminator, we break it down to two neural networks one for "real" data, one for "fake" data.  Therefore, in total we actually have three neural networks in our GAN.</p>
		<p>The real discriminator is a five layer convolutional neural network which takes the training data through its convolutional neural network, downsampling, in order to identify recognizable patterns in the training data.</p>
		<p>The fake discriminator is structured identically to the "real" counterpart, but utilizes the results of our generator neural network.</p>
		<p>The generator network is also strctured as a five layer neural network, but instead of doing downsampling, it actually begin with random weights then upsamples the data to generate an audio sample to mimic the apropriate bitrate and frequency of audio we're attempting to synthesize.</p>
		<p>The particular implementation is a WGAN (or Wasserstein GAN); with the WGAN, instead of the discriminator producing a classification, it produces a "distribution" of what a particular sample may look like.  So in our case, we have a distribution for "real" data as well as a distribution for "fake" data.  The aspect of the WGAN takes these two distributions and computes the Wasserstein distance between the two in order to determine if a sample is real or fake (which distribution does it fall into).  In this implementation, the Discriminator and Generator are part of a two player mini-max game.  The discriminator attempts to minimize the loss function (fake - real), whereas the generator attempts to maximize the loss function (for the fake results) thereby attempting to generate data which will “fool” the discriminator.</p>
	</div>
	
<div>
<<<<<<< HEAD
	
			 <section class="approach" id="wavegan">
    <h2 class="approach_header">Convolution </h2>
		 <div class="LeftTextArea" align="left">
		<p>In the case of WaveGAN, we actually apply an upsampling filter on our data.  Consider the diagram.  With each audio sample we actually increasing the dimensionality of the feature space and “filling in” with the data filtered content.</p>
		<p>The layers also conduct a “phase shuffle” which also helps introduce an element of periodicty in the data.</p>
  <p>These notions of convolution are same in principal but slightly different in implementation than some image recognition applications in which dimensionality is actually reduced.  This upsampling can actually create periodic patterns in our data, which is actually more remniscent of true audio harmonics.</p>
	</div>
  </section>
	
		
		
  		<div  style="display:block; vertical-align:middle; text-align:center; margin: 20px"> <img src="images/wavegan_convolution_audio.jpg" > </div>
		<div style="display:block; vertical-align:middle; text-align:center; argin: 20px" > <img src="images/phase_shuffle.png" width="515" height="515" alt=""/> </div>
	</div>

	<div>
		<section class="approach" id="wavegan">
    <h2 class="approach_header">Loss Analysis </h2>
		
		<div style="display:block; vertical-align:middle; text-align:center; margin: 20px"> <img src="images/kick_1_1_D_loss.png" width="775" height="250" alt=""/></div>
	 <div style="display:block; vertical-align:middle; text-align:center; margin: 20px">  <img src="images/kick_1_1_G_loss.png" width="775" height="250" alt=""/> </div>
			<div class="LeftTextArea" align="left">
<p>In some of our initial training runs, in this case with Kick Drums, we can compare the loss between the discriminator and generator neural networks.  This was after about 600 or so training steps, which based on our batch size only computed to about 50 epochs through the training set of 770 sound files and took about 5 days; furthermore the results were slow and audio was not spectacular.</p>
=======
		<h2>Convolution</h2>
		<p>Convolution within a neural network can be simplisticly seen as the application of a filter to the data, in order to generalize and extract features.</p>
		<p>This implementation uses two types of convolution: downsampling in our discriminators to filter and reduce dimensionality; and upsampling in our generator where we increase the dimensionality of our features to generate a sound sample.  In up sampling, consider the diagram, with each audio sample we actually increasing the dimensionality of the feature space and “filling in” with the data filtered content.  The NN layers also conduct a “phase shuffle” which also helps introduce an element of periodicty in the data.</p>
  		<p>These notions of convolution are same in principal but slightly different in implementation than some image recognition applications in which dimensionality is actually reduced.  This upsampling can actually create periodic patterns in our data, which is actually more remniscent of true audio harmonics.</p>
	    <img src="images/convolution_example.png" width="678" height="462" alt=""/> <img src="images/wavegan_convolution_audio.jpg" width="515" height="179" alt=""/>
		<img src="images/phase_shuffle.png" width="1046" height="1056" alt=""/>
	</div>

	<div>
	  <h2>Loss Analysis</h2>
		<img src="images/kick_1_1_D_loss.png" width="1562" height="880" alt=""/>
	  <img src="images/kick_1_1_G_loss.png" width="1550" height="872" alt=""/>
		<p>In some of our initial training runs, in this case with Kick Drums, we can compare the loss between the discriminator and generator neural networks.  This was after about 600 or so training steps, which based on our batch size only computed to about 50 epochs through the training set of 770 sound files and took about 5 days; furthermore the results were slow and audio was not spectacular.</p>
>>>>>>> 7520aa6c10a392c0b2a8b9bb0f5697090b74379a
		<p>The discriminator loss is erratic but seems to stabilize eventually, but still quite a ways from zero, where we would ideally like it to be where it’s able to accurately predict between real and fake samples.</p>
		<p>The generator loss is way off at the beginning but seems to stabilize closer to zero, it actually ends also at around -5, not too different from the discriminator, but not able to generate accurate sounds.</p>
			</div>
		</section>
	</div>

<div>
	<section class="approach" id="wavegan">
    <h2 class="approach_header">Training Challenges </h2>
		<div class="LeftTextArea" align="left">
		<p>We attempted to generate better samples by using larger batch sizes, using spliced audio samples to increase the size of our dataset which did not have significantly better audio generation.  What ultimately needed to happen was improving our algorithm efficiency to train faster.</p>
  <p>Based on our challenges we improve our process by pre-processing our audio data to have consistent frequency and bit rates.  Training with this allowed us to use a more efficient python library and quickly process our data through the neural network.</p>
  
		</div>
		
		 <div style="display:block; vertical-align:middle; text-align:center; margin: 20px"><img src="images/kick_2_0_D_loss.png" width="775" height="250" alt=""/> </div>
	 <div style="display:block; vertical-align:middle; text-align:center; margin: 20px"> <img src="images/kick_2_0_G_loss.png" width="775" height="250" alt=""/> </div>
	</section>
	</div>
	
<div>
	
	<section class="approach" id="wavegan">
    <h2 class="approach_header">Additional Post Processing </h2>

		<div style="display:block; vertical-align:middle; text-align:center; margin: 20px">  <img src="images/real_waveform.png" width="775" height="250" alt=""/> </div>
	 <div style="display:block; vertical-align:middle; text-align:center; margin: 20px">   <img src="images/generated_waveform.png" width="775" height="250" alt=""/> </div>
		<div class="LeftTextArea" align="left">
<p>Our audio analysis showed us a very "digital" output, that is our wave forms resembled step functions.  To mitigate this, we conducted post processing on our generated audio using a low pass filter algorithm that attenuates our wave forms against high changes in frequency and pitch.  We additional applied a moving average filter which conducts further attenuation to help remove some of the reverberation in the audio which we generate.</p>
		</div>
	</section>
</div>

	<div>
		<section class="approach" id="wavegan">
    <h2 class="approach_header">High Level Flow </h2>

		<div style="display:block; vertical-align:middle; text-align:center; margin: 20px"> 	  <img src="images/overall_process.jpg" width="536" height="274" alt=""/> </div>

	<div class="LeftTextArea" align="left">
		<p>We begin by taking our raw audio data perform frequency and bitrate conversion so they’re all consistent (44100Hz and 16bit rate)  This is accumualted as out training data we use in our GAN</p>

		<p>We train our GAN use the generative model to generate our new sounds for our application, but to improve quality, we run additional algorithms such as applying a low pass filter and wave form smooth (using moving average) to make our synthetic audio sound more natural.</p>
		</div>
</section>
	</div>
	
	<div>
		<section class="approach" id="wavegan">
    <h2 class="approach_header">References/Citations </h2>
			<div class="LeftTextArea" align="left">
		Adversarial Audio Synthesis<br>
		Donahue, Chris and McAuley, Julian and Puckette, Miller<br>
		ICLR 2019<br>
		https://github.com/chrisdonahue/wavegan<br>
			</div>
		</section>
	</div>
</body>
</html>
